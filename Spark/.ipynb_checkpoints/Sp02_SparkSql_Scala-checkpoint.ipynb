{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregations\n",
    "\n",
    "The built-in DataFrames functions provide common aggregations such as count(), countDistinct(), avg(), max(), min(), etc. While those functions are designed for DataFrames, Spark SQL also has type-safe versions for some of them in Scala and Java to work with strongly typed Datasets. Moreover, users are not limited to the predefined aggregate functions and can create their own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@167920fa\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@167920fa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object MyAverage\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "object MyAverage extends UserDefinedAggregateFunction {\n",
    "  // Data types of input arguments of this aggregate function\n",
    "  def inputSchema: StructType = StructType(StructField(\"inputColumn\", LongType) :: Nil)\n",
    "  // Data types of values in the aggregation buffer\n",
    "  def bufferSchema: StructType = {\n",
    "    StructType(StructField(\"sum\", LongType) :: StructField(\"count\", LongType) :: Nil)\n",
    "  }\n",
    "  // The data type of the returned value\n",
    "  def dataType: DataType = DoubleType\n",
    "  // Whether this function always returns the same output on the identical input\n",
    "  def deterministic: Boolean = true\n",
    "  // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to\n",
    "  // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides\n",
    "  // the opportunity to update its values. Note that arrays and maps inside the buffer are still\n",
    "  // immutable.\n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = 0L\n",
    "    buffer(1) = 0L\n",
    "  }\n",
    "  // Updates the given aggregation buffer `buffer` with new input data from `input`\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "    if (!input.isNullAt(0)) {\n",
    "      buffer(0) = buffer.getLong(0) + input.getLong(0)\n",
    "      buffer(1) = buffer.getLong(1) + 1\n",
    "    }\n",
    "  }\n",
    "  // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)\n",
    "    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)\n",
    "  }\n",
    "  // Calculates the final result\n",
    "  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyAverage$@6a6279a"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register the function to access it\n",
    "spark.udf.register(\"myAverage\", MyAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [name: string, salary: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, salary: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"./data/employees.json\")\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|average_salary|\n",
      "+--------------+\n",
      "|        3750.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [average_salary: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[average_salary: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.sql(\"SELECT myAverage(salary) as average_salary FROM employees\")\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
